\documentclass[../stats.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}
\begin{document}
\chapter{Probability, Random Variables, and Probability Distributions}
\section{Basic Probability and Simulations}
\begin{itemize}
    \item Outcomes are governed by change, but in many repetitions, a pattern emerges.
    \item Sample space: all of the possible outcomes.
    \item Event: a specific desired outcome or set of outcomes.
    \item Notation: probability of Event A $\rightarrow$ P(A).
    \item Range of probabilities: probability of event is between 0 and 1.
    \item Sum of probabilities: probability of whole sample is 1.
\end{itemize}

Theoretical probability is what should happen given the sample space: \# of desired outcomes/total \# outcomes.

Empirical probability is when you are performing a simulation or experiment, it is what does happen given the trials: \# successes/\# trials.

The law of large numbers states that simulated probabilities tend to get closer to the true probability as the number of trials increases. 

The notation for the probability that an event does not occur is $P(A^C)$. The probabiliy an event does not occur is $P(A^C)=1-P(A)$.
\begin{itemize}
    \item The idea of probability is that randomness is predictable in the long run.
    \item Probability does not allow us to make short run predictions.
    \item Probability tells us random behavior events out in the long run.
    \item Future outcomes are not affected by past behavior.
\end{itemize}

The imitation of chance behavior, based on a model that accurately reflects the situation, is called a simulation. Simulations are usually 
done with a table of random digits, random number generator, dice, deck of cards, spinner, etc.

Four principles of simulation:
\begin{itemize}
    \item State: identify the probability calculation.
    \item Plan: Describe how to use your chance process.
    \item Do: Perform the simulation 
    \item Conclude: use the results of your simulation to answer the question.
\end{itemize}
\section{The Addition Rule}
\begin{itemize}
    \item When two events have no outcomes in common, we refer to them as mutually exclusive events.
    \item P(A and B) = 0
    \item P(A or B) = P(A)+P(B)
\end{itemize}

If A and B are any two events resulting from some chance process, then
\begin{center}
    P(A or B) = P(A) + P(B) - P(A and B)
\end{center}
Notice how if A and B are mutually exclusive, P(A and B) = 0 and P(A or B) = P(A) + P(B).


\section{Venn Diagrams and the Multiplication Rule}
\begin{itemize}
    \item We can represent events with a Venn diagram - a display of potential probabilities.
    \item The box around the Venn diagram represents the total sample space where P(S) = 1.
    \item The circles themselves represent the probability of each event.
\end{itemize}

Union: $\cup$ is the probability that either occurs.

Intersection: $\cap$ is the probability both will occur.

\begin{itemize}
    \item Two events are independent if they do not influence one another.
    \item The occurrence of one has no effect on the occurrence of the other.
    \item If A and B are independent events, then the probability that both A and B occur is found using the multiplication rule:
    \begin{center}
        P(A and B) = P(A $\cup$ B) = P(A)$\cdot$P(B)
    \end{center}
\end{itemize}

\begin{itemize}
    \item Two events are dependent if they influence one another.
    \item The occurrence of one affects the occurrence of the other.
    \item If A and B are dependent events, then the probability that both A and B occur is found using:
    \begin{center}
        P(A and B) = P(A $\cup$ B) = P(A)$\cdot$P(B$|$A)
    \end{center}
    \item Where P(B$|$A) is the probability B ``given'' that A has occurred.
    \item The conditional probability of an event is the probability that one event will happen, if it is known that another event has happened.
\end{itemize}

\section{Conditional Probability and Tree Diagrams}
\begin{itemize}
    \item If A and B are dependent events, then the probability that both A and B occur is found using 
    P(A$\cup$B)=P(A)$\cdot$P(B$|$A) where P(B$|$A) is the probability of B ``given'' that A has already occurred.
    \item The conditional probability of an event is the probability that one event will happen, if it is known that another event has happened.
    \item We can rearrange the multiplication rule a little and get a formula for conditional probability.
    \begin{center}
        P(B|A)=$\frac{\text{P(A$\cup$ B)}}{\text{P(A)}}$
    \end{center}
\end{itemize}
\begin{itemize}
    \item If we draw two cards with replacement, we know that those events are independent.
    \item If we draw two cards without replacement, we know that those events are dependent.
    Using the following formula we can prove independence:
    \begin{center}
        P(A$\cup$B) = P(A)$\cdot$P(B) or P(A) = P(A$|$B)
    \end{center}
\end{itemize}
\section{Discrete and Continuous Random Variables}
A random variable takes numerical values that describe the outcomes of some chance process. Random variables involve the same probability rules 
we have already learned, but we will extend those rules to be able to model more events.

A probability distribution tells us the value that our random variable can take and the probability associated with each value.

Every probability distribution must satisfy each of the following requirements.
\begin{enumerate}
    \item There is a numerical random variable X, and each of the values of X has an associated probability with it.
    \item The sum of all the probabilities in the distribution $\sum P(x)$ must equal 1.
    \item $0\leq P(x)\leq 1$ for all probabilities in the distribution.
\end{enumerate}

A discrete random variable takes on a fixed set of possible values with whole number outcomes.
\begin{itemize}
    \item Random variables are usually capital letters.
    \item Random variables can be discrete or continuous 
    \item Random variables must be numeric in value 
\end{itemize}
The mean of a discrete random variable X, is the mean outcome for infinitely many trials.

We think of this as the ``expected'' value because it is the average value we would expect to get if the trials could continue indefinitelyindefinitely.

To find the mean, $\mu_x$, or expected value, $E(x)$, of a discrete random variable X, multiply each possible value by its probability, then add all the products.
\[E(x)=\sum x_i\cdot p_i\]

The standard deviation of a random variable X is a measure of how much the values of the variable typically vary from the expected value.
\[\sigma_x^2 = Var(X)=\sum(x_i-\mu_x)^2\cdot p_i\]
\[\sigma_x = \sqrt{\sum(x_i-\mu_x)^2\cdot p_i}\]

\begin{itemize}
    \item Continuous Random Variables take on all possible values in an interval of numbers. The probability distribution of X is described by a density curve.
    \item The probability of any event is the area under the density curve and above, below, or between the values x that define the event.
    \item Area under a density curve is always equal to 1.
    \item The probability is on the y-axis in a distribution, so finding the area equates to finding the probability of getting an interval of numbers.
    \item The probability at an event is 0 for a continuous random variable because the area under a point is 0.
\end{itemize}

Calculator: Mean and Standard Deviation from a Probability Distribution Table 
\begin{itemize}
    \item Enter outcomes into L1 and probabilities in L2 
    \item Run 2nd - Vars - 1VarStats with List:L1 and FreqList:L2 
    \item Mean will be represented by $\overline{x}$
    \item Standard deviation will be represented by $\sigma_x$
\end{itemize}

\section{Combining Random Variables}
If $X$ and $Y$ are two independent random variables:

\[E(X+Y)=E(X)+E(Y)\]
\[E(X-Y)=E(X)-E(Y)\]
\[\sigma_{X+Y}=\sqrt{\sigma_x^2 + \sigma_y^2}\]
\[\sigma_{X_Y}=\sqrt{\sigma_x^2 + \sigma_y^2}\]

If $X$ is a random variable and $a$ and $b$ are both constants:
\[E(a+bX)=a+bE(x)\]
\[\sigma_{a+bX}=|b|E(x)\]

\section{The Binomial Distribution}
Requirements for a Bernoulli Trial:
\begin{enumerate}
    \item Two Possible Outcomes 
    \item Probability of Success is the Same for Each Trial 
    \item Trials are Independent 
\end{enumerate}

If we take a Bernoulli trial and then we are interested in the number of successes in a specific number of trials, we create a Binomial Probability Distribution.

There are four requirements for a setting to follow the binomial probability distribution. As long as these four things are true, we can use the binomial probability model for calculating the probabilities.

\begin{itemize}
    \item BINARY: Each trial falls into one of two categories - we call them ``success'' or ``failure''. Success does not necessarily mean a positive outcome, but instead, the outcome we are looking for.
    \item Independent Trials
    \begin{itemize}
        \item Each trial is independent of the next 
        \item Reasonable assumption for coins, cards with replacement, spinners, rolling a die 
        \item What about sampling without replacement, when it can't be avoided? This is where the ``10\% condition'' comes in. The 10\% condition says that if you are sampling from a large enough population, you can proceed with sampling without replacement as though the trials were independent.
    \end{itemize}
    \item Number of Trials is fixed: we are counting the number of successes from a set number of trials (called $n$)
    \item Same probability: The probability of successes (called $p$) is the same for each trial 
\end{itemize}

If all four points are satisfied, you can compute the probability you get a certain number of successes in a specified number of trials:

$n$ is the number of independent trials, $p$ is the probability of successes, and $x$ is the number of successes. 

\[ P(X=x) = \binom{n}{x}p^x(1-p)^{n-x} \]
where $\binom{n}{x}=nCx = \frac{n!}{x!(n-x)!}$

The mean is $\mu_x = np$ and the standard deviation is $\sigma_x = \sqrt{np(1-p)}$.

Distributions in your calculator often have ``pdf'' and ``cdf''.

pdf is the probability distribution function and is used when trying to calculate $P(X=k)$ and cdf is the cumulative distribution function and is used when you are trying to calculate $P(X\leq k)$.

Specifically for Binomial distributions we want 
\begin{itemize}
    \item 2nd - VARS - A: binompdf(trials, probability of success, k) for $P(X=k)$
    \item 2nd - VARS - B: binomcdf(trials, probability of success, k) for $P(X\leq k)$
\end{itemize}
\section{The Geometric Distribution}
WHen we perform many independent trials of the same chance process and are interested in the occurrence of the first success, a geometric setting arises.

If $X$ has a geometric distribution with a probability of success $p$ on each trial and $x$ represents the trial that you get your first success, the probability that 
$X$ equals $x$ is given by 
\[ P(X=x) = (1-p)^{x-1}(p) \]

Specifically for geometric distributions we want... 
\begin{itemize}
    \item 2nd - Vars - E: geometpdf(probability of success, k) for $P(X=x)$
    \item 2nd - Vars - F: geometcdf(probability of success, k) for $P(X\leq x)$
\end{itemize}
For this distribution, $\mu_x = \frac{1}{p}$ and $\sigma_x = \frac{\sqrt{1-p}}{p}$
\end{document}
