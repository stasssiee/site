\documentclass[../linalg.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}
\begin{document}
\chapter{Vector Spaces}
\section{Vector Spaces and Subspaces}
A vector space is a nonempety set $V$ of objects, called vectors, on which are defined two operations called addition and scalar multiplication to the 10 axioms listed below. The axioms must hold for all vectors $\textbf{u}$, $\textbf{v}$, and $\textbf{w}$ in $V$ and for all scalars $a$ and $b$.

Closure Properties:
\begin{itemize}
    \item $\textbf{u}+\textbf{v}$ is a vector in $V$
    \item $a\textbf{v}$ is a vector in $V$
\end{itemize}

Properties of Addition:
\begin{itemize}
    \item commutative: $\textbf{u}+\textbf{v}=\textbf{v}+\textbf{u}$
    \item associative: $\textbf{u}+(\textbf{v}+\textbf{w})=(\textbf{u}+\textbf{v})+\textbf{w}$
    \item additive identity: there is a vector $\textbf{0}$ in $V$ such that $\textbf{v}+\textbf{0}=\textbf{v}$ for all $\textbf{v}$ in $V$
    \item additive inverse: given a vector $\textbf{v}$ in $V$, there is a vector $-\textbf{v}$ such that $\textbf{v}+(-\textbf{v})=\textbf{0}$
\end{itemize}

Properties of Scalar Multiplication:
\begin{itemize}
    \item associative: $a(b\textbf{v})=(ab)\textbf{v}$
    \item distributive: $a(\textbf{u}+\textbf{v})=a\textbf{u}+a\textbf{v}$
    \item distributive: $(a+b)\textbf{v}=a\textbf{v}+b\textbf{v}$
    \item multiplicative identity: $1\textbf{v}=\textbf{v}$ for all $\textbf{v}$ in $V$
\end{itemize}

A subspace of a vector space, $V$, is a subset $H$ of $V$ that has three properties:
\begin{enumerate}
    \item the zero vector of $V$ is $H$
    \item $H$ is closed under vector addition 
    \item $H$ is closed under scalar multiplication
\end{enumerate}

A subspace $H$ of $V$ itself is a vector space. The other properties of a vector space are ``inherited'' since $H$ is a subset of $V$.

\begin{theorem}
    If $\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p$ are in a vector space $V$, then Span$\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$ is a subspace of $V$.
\end{theorem}

\section{Null Spaces, Column Spaces, Row Spaces, and Linear Transformations}
The Null Space of a $m\times n$ matrix $A$, denoted Nul $A$, is the set of solutions of the homogeneous equation $A\textbf{x}=\textbf{0}$. In set notation: Nul $A$ = $\{\textbf{x}:\textbf{x}$ is in $\textbf{R}^n$ and $A\textbf{x}=\textbf{0}\}$.
\begin{theorem}
    The null space of an $m\times n$ matrix $A$ is a subspace of $\textbf{R}^n$. Equivalently, the set of all solutions to a system $A\textbf{x}=\textbf{0}$ of $m$ homogeneous linear equations in $n$ unknown is a subspace of $\textbf{R}^n$.
\end{theorem}

The Column Space of a $m\times n$ matrix $A$, denoted Col $A$, is the set of all linear combinations of the columns of $A$. If $A=[\textbf{a}_1,\textbf{a}_2,\dots, \textbf{a}_n]$ then Col $A$ = Span$\{\textbf{a}_1,\textbf{a}_2,\dots,\textbf{a}_n\}$.

\begin{theorem}
    The column space of a $m\times n$ matrix $A$ is a subspace of $\textbf{R}^m$.
\end{theorem}

The Row Space of an $m\times n$ matrix $A$, denoted Row $A$, is the set of all linear combinations of the row vectors. Each row has $n$ entries, so the Row $A$ is a subspace of $\textbf{R}^n$.

The null and column space are related. They are very different: When $A$ Is not square, the column space and the null space exist entirely in different ``universes''. For an $m\times n$ matrix, the Column Space is in $m$-dimensional space, where the Null space is in $n$-dimensional space.

Kernel and Range if a Linear Transformation
\begin{itemize}
    \item a Linear Transformation $T$ from a vector space $V$ to be vector space $W$ is a rule that assigns each vector $\textbf{x}$ in $V$ a unique vector $T(\textbf{x})$ in $W$ such that $T(\textbf{u}+\textbf{v})=T(\textbf{u})+T(\textbf{v})$ and 
    $T(c\textbf{u})=cT(\textbf{u})$ for all $\textbf{u},\textbf{v}$ in $V$ and $c$ in $\textbf{R}$.
    \item The Kernel of $T$ is the set of all $\textbf{u}$ in $V$ such that $T(\textbf{u})=\textbf{0}$ 
    \item The Range of $T$ is the set of all vectors in $W$ of the form $T(\textbf{x})$ for some $\textbf{x}$ in $V$
    \item The kernel and range of $T$ are subspaces of $V$
\end{itemize}



\section{Linearly Independent Sets; Bases}
A basis spans a vector space as ``efficiently'' as possible.
\begin{itemize}
    \item linearly independent: an indexed set of vectors $\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$ in $V$ is linearly independent if $c_1\textbf{v}_1+c_2\textbf{v}_2+\dots+c_n\textbf{v}_n=\textbf{0}$ has only the trivial solution.
    \item linearly dependent: an indexed set of vectors $\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$ in $V$ is linearly dependent if $c_1\textbf{v}_1+c_2\textbf{v}_2+\dots+c_n\textbf{v}_n=\textbf{0}$ has a nontrivial solution 
\end{itemize}

\begin{theorem}
    An indexed set of vectors $\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$ of 2 or more vectors with $\textbf{v}_1$ not zero, is linearly dependent if and only if some $\textbf{v}_j(j>1)$ is a linear combination of the preceding vectors 
    $\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_{j-1}$.

    This definition is for general vectors where we may be unable to write the vectors as columns of a matrix $A$.
\end{theorem}

Basis: Let $H$ be a subspace of a vector space $V$. A set of vectors $\mathcal{B}$ in $V$ is a basis of $H$ if:
\begin{enumerate}
    \item $\mathcal{B}$ is a linearly independent set and 
    \item the subspace spanned by $\mathcal{B}=H$ (or $H$ = span $\mathcal{B}$)
\end{enumerate}

\begin{theorem}
    $S=\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$ is a set in a vector space, and let $H$ = Span$\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$.
    \begin{itemize}
        \item if one of the vectors in $S$, say $\textbf{c}_k$, is a linear combination of the remaining vectors in $S$, then the set formed from $S$ by removing $\textbf{v}_k$ still spans $H$.
        \item if $H$ is not equal to $\{\textbf{0}\}$, some subset of $S$ is a basis for $H$
    \end{itemize}
\end{theorem}

The basis if the smallest possible spanning set (because all vectors are linearly independent)

The basis is the largest possible linearly independent set that spans (an additional vector will make the set dependent)



\section{Coordinate Systems}
\begin{theorem}
    Let $\mathcal{B}=\{\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n\}$ be a basis for a vector space $V$. Then for each $\textbf{x}$ in $V$, there exists a unique set of scalars $c_1,c_2,\dots, c_n$ such that $\textbf{x}=c_1\textbf{b}_1+c_2\textbf{b}_2+\dots+c_n\textbf{b}_n$.
\end{theorem}
Coordinates: suppose $\mathcal{B}=\{\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n\}$ be a basis for a vector space $V$. The coordaintes of $\textbf{x}$ relative to the basis $\mathcal{B}$ are the weights $c_1,c_2,\dots,c_n$ such that $\textbf{x}=c_1\textbf{b}_1+c_2\textbf{b}_2+\dots+c_n\textbf{b}_n$.

The change of coordinates matrix, $P_{\mathcal{B}}=[\textbf{b}_1 \textbf{b}_2 \dots \textbf{b}_n]$ change coordinates from $\mathcal{B}$ to the standard basis in $\textbf{R}^n$.
$P_{\mathcal{B}}^{-1}$ transforms $\textbf{x}$ to $[\textbf{x}]_{\mathcal{B}}$.

\begin{theorem}
    Let $\mathcal{B}=\{\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n\}$ be a basis for a vector space $V$. Then the coordinate mapping $\textbf{x} -> [\textbf{x}]_{\mathcal{B}}$ is a one-to-one, onto linear transformation from $V$ to $\textbf{R}^n$.
\end{theorem}

The coordinate mapping in this theorem is an important example of isomorphism from $V$ onto $\textbf{R}^n$. The notation and terminology for the two vector space may be different, but the two spaces are indistinguishable. Every vector space calculation in $V$ is accurately reproduced in $\textbf{R}^n$ and vice versa.




\section{The Dimension of a Vector Space}
\begin{theorem}
    If a vector space $V$ has a basis $\mathcal{B} = [\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n]$ then any set in $V$ containing more than $n$ vectors must be linearly dependent.
\end{theorem}

\begin{theorem}
    If a vector space $V$ has a basis $\mathcal{B}= [\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n]$ then every basis of $V$ must contain exactly $n$ vectors.
\end{theorem}

Dimension: if a vector space $V$ is spanned by a finite space, then $V$ is said to be finite-dimensional, and the dimension of $V$, dim $V$ is the number of vectors in a basis for $V$. The dimension of the zero vector 
space $\{\textbf{0}\}$ is defined to be zero. If $V$ is not spanned by a finite set, then $V$ is said to be infinite-dimensional. 

\begin{theorem}
    Let $H$ be a subspace of a finite dimensional vector space $V$. Any linearly independent set in $H$ can be expanded, if necessary, to be a basis for $H$. Also, $H$ is finite dimensional and dim H $\leq$ dim V 
\end{theorem}

\begin{theorem}
    Let $V$ be a $p$-dimensional vector space with $p\geq 1$. Any linearly independent set of exactly $p$ elements in $V$ is a basis for $V$. Any set of exactly $p$ elements that spans $V$ is a basis for $V$
\end{theorem}

The rank of an $m\times n$ matrix $A$ is the dimension of the column space, and the nullity of $A$ is the dimension of the null space.
\begin{theorem}
    The dimension of the column space and the null space of an $m\times n$ matrix $A$ satisfies the equation: rank $A$ + nullity $A$ = number of columns in $A$
\end{theorem}
The rank of an $m\times n$ matrix $A$ is the number of pivot columns and nullity of $A$ is the number of free variables. Since the dimension of the row space is the number of pivot rows, dim row space = rank $A$.

The following are equivalent to the statement $A$ is an invertible matrix:
\begin{itemize}
    \item the columns of $A$ for a basis for $\textbf{R}^n$
    \item Col $A$ = $\textbf{R}^n$
    \item rank $A$ = $n$
    \item nullity $A$ = 0
    \item Nul A = $\{0\}$
\end{itemize}

\section{Change of Basis}
\begin{theorem}
    Let $\mathcal{B}=\{\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n\}$ and $\mathcal{B}'=\{\textbf{b}_1',\textbf{b}_2',\dots,\textbf{b}_n'\}$ be bases of a vector space $V$.

    Then there is a unique $n\times n$ matrix $\textbf{P}_{\mathcal{B}\leftarrow \mathcal{B}}$ such that $[\textbf{x}]_{\mathcal{B}}=\textbf{P}_{\mathcal{B}\leftarrow \mathcal{B}}[\textbf{x}]_{\mathcal{B}}$. The columns of 
    $\textbf{P}_{\mathcal{B}\leftarrow \mathcal{B}}$ are the $\mathcal{B}'$ coordinates vectors of the vectors in the basis $\mathcal{B}$. 
\end{theorem}

If $\mathcal{B}=\{\textbf{b}_1,\textbf{b}_2,\dots,\textbf{b}_n\}$ and $\mathcal{E}$ is the standard basis $\{\textbf{e}_1,\textbf{e}_2,\dots,\textbf{e}_n\}$ then $[\textbf{b}_1]_{\mathcal{E}}=\textbf{b}_1$, and likewise for the other vectors in $\mathcal{B}$.

\end{document}