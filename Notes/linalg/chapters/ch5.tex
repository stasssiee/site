\documentclass[../linalg.tex]{subfiles}
\graphicspath{{\subfix{../figures/}}}
\begin{document}
\chapter{Eigenvalues and Eigenvectors}
\section{Eigenvectors and Eigenvalues}
Eigenvectors and Eigenvalues: viewing a matrix as a linear transformation $x\rightarrow A\textbf{x}$ vectors that are only scaled (not rotated) are called eigenvectors. The scaling factor is the associated eigenvalue.

\begin{definition}
    An eigenvector of an $n\times n$ matrix $A$ is a nonzero vector $\vec{x}$ such that 
    \[ A\vec{x} = \lambda \vec{x} \]
\end{definition}
is true for some scalar $\lambda$. A scalar $\lambda$ is called an eigenvalue of $A$ if there is a nontrivial solution $\vec{x}$ of the above equation. We often state that 
$\vec{x}$ is an eigenvector corresponding to $\lambda$.

The equation you will solve to find eigenvectors: $A\vec{x}=\lambda\vec{x}\implies A\vec{x}-\lambda\vec{x}=\vec{0}\implies (A-\lambda I)\vec{x}=\vec{0}$.

The set of solutions to this homogeneous equation is just the nullspace of the matrix, so this set is a subspace of $\textbf{R}^n$ and is caleld the eigenspace corresponding to $\lambda$.

\begin{theorem}
    The eigenvalues of a triangular matrix are the entries on its main diagonal.
\end{theorem}

\begin{theorem}
    If $\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p$ are eigenvectors that correspond to distinct eigenvalues of an $n\times n$ matrix, then the set 
    $\{\textbf{v}_1,\textbf{v}_2,\dots,\textbf{v}_p\}$ is linearly independent.
\end{theorem}

Eigenvectors and Difference Equations: constructing a solution of the first-order difference equation: $x_{k+1}=A\textbf{x}_k (k=0,1,2,\dots)$.

If $A$ is $n\times n$ then $\textbf{x}_{k+1} (k=0,1,2,\dots)$ is a recursive description of a sequence $\{\textbf{x}_k\}$ in $\textbf{R}^n$.

A solution is an explicit description of $\{\textbf{x}_k\}$ whose formula for each $\textbf{x}_k$ does not depend directly on $A$ or the preceding terms in the sequence other than the initial term $\textbf{x}_0$.

The simplest way to build a solution is to take an eigenvector $\textbf{x}_0$ and it's corresponding eigenvalue and let $\vec{x}_k=\lambda^k x$.

This sequence is a solution because: $A\vec{x}_k=A(\lambda^k\vec{x}_0)=\lambda^k(A\vec{x}_0)=\lambda^k(\lambda \vec{x}_0)=\lambda^{k+1}\vec{x}_0=\vec{x}_{k+1}$.


\section{The Characteristic Equation}
\begin{theorem}
    An $n\times n$ matrix $A$ is invertible if and only if 0 is not an eigenvalue of $A$,
\end{theorem}

The Characteristic Equation: a scalar $\lambda$ is an eigenvalue of an $n\times n$ matrix $A$ if and only if $\lambda$ satisfies the characteristic equation: det $(A-\lambda I)=0$.

The characteristic equation of an $N\times n$ matrix is an $n$th degree polynomial. We expect exactly $n$ roots, counting multiplicities, provided complex roots are allowed.

Similarity: If $A$ and $B$ are $n\times n$ matrices, then $A$ is similar to $B$ if there is an invertible matrix $P$ such that $P^{-1}=AP=B$ or equivalently $A=PBP^{-1}$. Changing $A$ to $P^{-1}AP$ is called a similarity transformation.

\begin{theorem}
    If $n\times n$ matrices $A$ and $B$ are similar, then they have the same characteristic polynomial and hence the same eigenvalues with the same multiplicities. 

    Similarity is not the same as row equivalence. If matrices have the same eigenvalues, they may not be similar.
\end{theorem}

\section{Diagonalization}
A square matrix $A$ is said to be diagonalizable if $A$ is similar to a diagonal matrix, that is if 
$A=PDP^{-1}$ for some invertible matrix $P$ and some diagonal, matrix $D$.

\begin{theorem}
    An $n\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.

    In fact, $A=PDP^{-1}$, with $D$ a diagonal matrix, if and only if the columns of $P$ are $n$ linearly independent eigenvectors of $A$.
    In this case, the diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively, to the eigenvectors in $P$.

    In other words, $A$ is diagonalizable if and only if there are enough eigenvectors to form a basis. We call such a basis an eigenvector basiso f.
\end{theorem}

\begin{theorem}
    An $n\times n$ matrix with $n$ distinct eigenvalues is diagonalizable.
\end{theorem}

\begin{theorem}
    Let $A$ be an $n \times n$ matrix whose distinct eigenvalues are $\lambda_1,\dots,\lambda_p$.

    For $1\leq k\leq p$, the dimension of the eigenspace for $\lambda_k$ is less than or equal to the multiplicity of the eigenvalue $\lambda_k$.

    The matrix $A$ is diagonalizable if and only if the sum of the dimensions of the eigenspaces equals $n$, and this happens if and only if the characteristic polynomial factors completely into linear factors and the dimension of the eigenspace for each 
    $\lambda_k$ equals the multiplicity of $\lambda_k$.

    If $A$ is diagonalizable and $B_k$ is a bsis for the eigenspace corresponding to $\lambda_k$ for each $k$, then the total collection of vectors in the sets $B_1,\dots,B_p$ forms and eigenvector basis.
\end{theorem}

\section{Applications to Differential Equations}
Systems of Differential Equations: Several quantities are varying continuously in time and related by a linear system of differential equations:

Linear $\textbf{x}'(t)=A\textbf{x}(t)$ where $\textbf{x}(t)=(x_1(t),x_2(t),\dots,x_n(t))$.

A solution of $\textbf{x}'(t)=A\textbf{x}(t)$ is a vector-valued function that satisfies $\textbf{x}'(t)=A\textbf{x}(t)$ for all $t$ in some interval of real numbers such as $t\geq 0$.

From Differential Equations, we know there always exist what is called a Fundamental Set of Solutions to $\textbf{x}'(t)=A\textbf{x}(t)$ A fundamental set if a basis for the set of all solutions of $\textbf{x}'(t)=A\textbf{x}(t)$, and the solution set is an 
$n$-dimensional vector space of functions.

If a vector $\textbf{x}_0$ specified, then the initial value problem is to construct the unique function $\textbf{x}(t)$ such that $\textbf{x}'(t)=A\textbf{x}(t)$ and $\textbf{x}(0)=\textbf{x}_0$.

The 2 real solutions to $\textbf{x}'(t)=A\textbf{x}(t)$ are: $\textbf{y}_1(t)=\text{Re}\textbf{x}_1(t)=[(\text{Re}\textbf{v})\cos (bt)-(\text{Im}\textbf{v})\cos (bt)]e^{at}$. and $\textbf{y}_2(t)$ is basically the same but it is Im $\textbf{x}_1(t)$.

\end{document}